{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% import libraries\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# custom defined functions\n",
    "from include.bow import dictionary, one_hot\n",
    "from include.io import import_captions, import_images, output_captions\n",
    "# style seaborn for plotting\n",
    "# %matplotlib qt5 (for interactive plotting: run in the python console)\n",
    "from include.networks.network import get_network_siamese, get_network_siamese_contrastive, get_network_triplet_loss\n",
    "# to quickly reload functions\n",
    "from include.training.dataset import convert_to_dataset, convert_to_triplet_dataset\n",
    "from include.util.pairs import get_pairs_images, make_dict\n",
    "\n",
    "sns.set()\n",
    "# print numpy arrays in full\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# %%  import data\n",
    "\n",
    "\n",
    "# caption_filename = '/home/kriekemans/KUL/information_retrieval/dataset/results_20130124.token'\n",
    "# image_filename = '/home/kriekemans/KUL/information_retrieval/dataset/image_features.csv'\n",
    "\n",
    "caption_filename = '../data/results_20130124.token'\n",
    "image_filename = '../data/image_features.csv'\n",
    "# read in data\n",
    "captions = import_captions.import_captions(caption_filename)\n",
    "images = import_images.import_images(image_filename)\n",
    "\n",
    "print('loaded {} captions'.format(len(captions)))\n",
    "print('loaded {} images'.format(len(images)))\n",
    "\n",
    "# %% create captions to bow dictionary\n",
    "bow_dict = dictionary.create_dict(captions)\n",
    "\n",
    "# %%\n",
    "# get pandas dataframe with\n",
    "# most frequent and least frequent words and visualize\n",
    "\n",
    "# %%\n",
    "# get stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# prune dictionary\n",
    "bow_dict_pruned, removed_words = dictionary.prune_dict(word_dict=bow_dict,\n",
    "                                                       stopwords=stop_words,\n",
    "                                                       min_word_len=3,\n",
    "                                                       min_freq=10,\n",
    "                                                       max_freq=1000)\n",
    "\n",
    "# have a look again at the most frequent words from the updated dictionary\n",
    "# _ = fw.rank_word_freq(dic=bow_dict_pruned, n=20, ascending=False, visualize=True)\n",
    "\n",
    "# have a look at the removed words\n",
    "# _ = fw.rank_word_freq(dic=removed_words, n=20, ascending=False, visualize=True)\n",
    "\n",
    "# %% # one hot encode\n",
    "tokens = list(bow_dict_pruned.keys())\n",
    "\n",
    "print('converting caption features')\n",
    "\n",
    "caption_feature_size = len(tokens)\n",
    "\n",
    "progress = 0\n",
    "pruned_captions = []\n",
    "for caption in captions:\n",
    "    progress += 1\n",
    "    if progress % 2500 == 0:\n",
    "        print(progress)\n",
    "    # efficiently store sparse matrix\n",
    "    # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "    caption.features = one_hot.convert_to_bow(caption, tokens)\n",
    "    pruned_captions.append(caption)\n",
    "    if (progress == 5000):\n",
    "        break\n",
    "# %%\n",
    "\n",
    "captions = pruned_captions\n",
    "\n",
    "print('features converted')\n",
    "#\n",
    "print('creating triplet dict')\n",
    "pair_dict = make_dict(images, captions)\n",
    "print('creating triplets')\n",
    "pairs = get_pairs_images(pair_dict)\n",
    "print('pairs created')\n",
    "print('creating dataset with labels')\n",
    "dataset, labels = convert_to_triplet_dataset(pairs)\n",
    "print('dataset created')\n",
    "print('network loading')\n",
    "network = get_network_triplet_loss(caption_feature_size,len(images[0].features), 256)\n",
    "print('network loaded')\n",
    "network.fit(dataset, labels, epochs=10, use_multiprocessing=True)\n",
    "print('network fitted')\n",
    "\n",
    "# print('features converted')\n",
    "#\n",
    "# print('creating pair dict')\n",
    "# pair_dict = make_dict(images, captions)\n",
    "# print('creating pairs')\n",
    "# pairs = get_pairs_images(pair_dict)\n",
    "#\n",
    "#\n",
    "# print('pairs created')\n",
    "# print('creating dataset with labels')\n",
    "# dataset, labels = convert_to_dataset(pairs)\n",
    "# print('dataset created')\n",
    "# print('network loading')\n",
    "# network = get_network_siamese(len(images[0].features), caption_feature_size, 256)\n",
    "# print('network loaded')\n",
    "#\n",
    "# print('input_2 -> {}'.format(dataset[1].shape))\n",
    "# network.fit(dataset, labels, batch_size=1)\n",
    "# print('network fitted')\n",
    "\n",
    "\n",
    "# %% output captions to compressed format + update captions.features\n",
    "output_captions.output_captions(captions=captions, tokens=tokens,\n",
    "                                file_name=\"include/data/caption_features.npz\",\n",
    "                                n_rows=len(captions))\n",
    "# # representation\n",
    "# print(captions[10].features)\n",
    "\n",
    "# # %% load caption features in compressed format\n",
    "#\n",
    "# df_captions = sparse.load_npz('include/data/caption_features.npz')\n",
    "# # if you want to go to the uncompressed format\n",
    "# # df_captions_uncomp = df_captions.todense()\n",
    "#\n",
    "# # %%images (normal format) (this is in pandas dataframe format) (31782, 2049)\n",
    "# df_image = pd.read_csv(\"include/data/image_features.csv\", sep=\" \", header=None)\n",
    "#\n",
    "# # TODO:\n",
    "# #   1) Define train/valdiation/test (and train_valdation ==> for training your final network)\n",
    "# #   2) Define architecture NN (keras)\n",
    "# #   3) loss functions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}